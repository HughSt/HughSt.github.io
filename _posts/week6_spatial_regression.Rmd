---
title: "Spatial Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 6 - Spatial Regression

## About this lecture

In this session we will introduce some concepts of spatial regression. We will focus on continuous spatial variation and see how to include it in a regression model. We will also discuss model selection using cross validation. The session is divided into the following sections:

* Linear Regression Models
* Spatial Covariance
* Geostatistics
* Cross-Validation
* Application Example: Malaria Case

Besides the code displayed here, we will use some additional code to generate some toy datasets that will help illustrate the exposition. Before starting we will load these code as well as the other required libraries, including ggplot2 which will be used for creating most of the images displayed.
```{r preliminary, echo=TRUE}
library(ggplot2)
library(fields)
library(raster)
library(sp)
library(spaMM)
source("https://raw.githubusercontent.com/HughSt/HughSt.github.io/master/course_materials/week6/Lab_files/R%20Files/background_functions.R")
```

## Linear Regression Models

### Univariate Linear Model

As a first step we will do a recap on a *linear regression model*. In this problem we have a set of measurments of two variables, say \(X\) and \(Y\), and we try to explain the values of \(Y\) based on the values on \(X\). To do this we find the line that is the closest to all the points \((x, y)\).

The command below generates a toy dataset that we will use as an example.
```{r dset1}
# Generate example data
dset1 <- univariate_lm()

# Show data
head(dset1)
```

In *R* we can fit a linear model and make predictions with the comands shown next.
```{r lm}
# Fit linear model on dataset 1
m1 <- lm(y ~ x, data = dset1)
m1_pred <- predict(m1, newdata = dset1, interval = "confidence")
dset1$y_hat <- m1_pred[,1]
dset1$y_lwr <- m1_pred[,2]
dset1$y_upr <- m1_pred[,3]
```

```{r plt_lm, echo=FALSE}
ggplot(data=dset1, aes(x, y)) + 
  geom_point(col="steelblue", size=2) + 
  geom_line(aes(x, y_hat), col="red") +
  geom_ribbon(aes(ymin=y_lwr, ymax=y_upr), fill="magenta", alpha=.25) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

### Univariate GLM

While very useful, it is common that the model above turns out to be a not good assumption. Think of the case where \(Y\) is constrained to be positive. A straight line, unless it is horizontal, will cross the \(y\)-axis at some point. If the values of \(X\) where \(Y\) becomes negative are rare or they are a set of values we are not interested in, we may simply ignore them, however there are scenarios where we cannot afford having impossible values for \(Y\).

As an example, we will load a second toy dataset and fit a liner regression model. Look at the bottom left corner of figure below. The predictions of \(Y\) are starting to cross the zero value and become negative, but the observed data remain positive.
```{r glm}
# Fit linear model on dataset 2
dset2 <- univariate_glm()
m2 <- lm(y ~ x, data = dset2)
m2_pred <- predict(m2, newdata = dset1, interval = "confidence")
dset2$y_hat <- m2_pred[,1]
dset2$y_lwr <- m2_pred[,2]
dset2$y_upr <- m2_pred[,3]

ggplot(data=subset(dset2), aes(x, y)) + 
  geom_point(col="steelblue", size=2) + 
  geom_line(aes(x, y_hat), col="red") +
  geom_ribbon(aes(ymin=y_lwr, ymax=y_upr), fill="magenta", alpha=.25) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

A solution to this problem is to use *generalized linear models* (GLM). A GLM uses a transformation on \(Y\) where the assumptions of the standard linear regression are valid (figure below), then it goes back to the original scale of \(Y\) and makes predictions.
```{r plt_logit, echo=FALSE}
ggplot(data=subset(dset2), aes(x, log(y/(1-y)))) + 
        geom_point(col="steelblue", size=2) + 
        theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
              panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

When fitting a GLM to the dataset shown in the second example above, the resulting predictions draw a curve that never reaches zero.
```{r glm_quasibinomial, echo=FALSE}
# Fit GLM to dataset 2
dset2 <- univariate_glm()
m2 <- glm(y ~ x, data = dset2, family = quasibinomial)
m2_pred <- predict.glm(m2, newdata = dset2, se.fit = TRUE, type="response")
dset2$y_hat <- m2_pred$fit
dset2$y_lwr <- m2_pred$fit - 1.96 * m2_pred$se.fit
dset2$y_upr <- m2_pred$fit + 1.96 * m2_pred$se.fit

ggplot(data=subset(dset2), aes(x, y)) + 
  geom_point(col="steelblue", size=2) + 
  geom_line(aes(x, y_hat), col="red") +
  geom_ribbon(aes(ymin=y_lwr, ymax=y_upr), fill="magenta", alpha=.25) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

### GLM with Spatially Structured Data

We will now move to a example of regression on spatial data. Say that we have a parcel of land where we are interested in quantifying the amount of organic matter. 
We take measurments at different locations randomly chosen, so that the locations can be any set of points in the parcel. We will also assume that there is a covariate \(X\), say humidity, measured at the same locations. 

The code below generates the data for this example and the figure shows such data. We are assuming that the organic matter is measuered in a fictitious scale where the unit is OM.
```{r toy_dataset}
# load toy dataset for example
spatial_reg <- soil_data(n_peaks=3, n_data = 300, seed=0)
head(spatial_reg)
```

```{r plt_soil_data, echo=FALSE}
# Plot soil data
ggplot(spatial_reg, aes(lng, lat)) + 
  geom_point(aes(col=OM), size=2.5) +
  viridis::scale_color_viridis(option="plasma") #+ theme_void()
```

The plot below shows the organic matter vs humidity. Notice that the values of organic matter are positive and that they become more spread the larger the values of humidity.
```{r plt_soil_cov, echo=FALSE}
# Plot organic matter vs humidity
ggplot(data=subset(spatial_reg), aes(humidity, OM)) + 
  geom_point(col="steelblue", size=2) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

If we transform the organic matter values with the logarithm, we get a clear linear relation with the values of humidity (see figure  below). This resembles a perfect straight line, because this is a toy example designed this way. Things are less clear in reality, but the principles shown here can still be applied to it.
```{r plt_logsoil_cova, echo=FALSE}
# Plot log organic matter vs humidity
ggplot(data=subset(spatial_reg), aes(humidity, log(OM))) + 
  geom_point(col="steelblue", size=2) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

In this case we will fit a GLM using the logarithm as link function. The model is described as

\(\log y_i = \eta_i = \beta_0 + \beta_1 x_i\),

where \(y_i\) is the amount of organic matter, \(x_i\) is the humidity level, and $\beta_0$ and \(\beta_1\) are parameters. To fit this model we can use the following command.
```{r glm_soil_cova}
# Fit GLM to toy dataset
m3 <- glm(OM ~ humidity, data=spatial_reg, family=gaussian(link="log"))
summary(m3)
```

Once we have our GLM fitted, we can analyze the residuals to check if the assumption of them being independent and identically distributed is valid. In the figure below it seems that the values of the residuals are spatially related. 
```{r plt_soil_cova, echo=FALSE}
# Plot residuals
spatial_reg$residuals <- residuals(m3)
ggplot(spatial_reg, aes(lng, lat)) + 
  geom_point(aes(col=residuals), size=2.5) +
  viridis::scale_color_viridis(option="plasma") #+ theme_void()
```

We will make a more objective assesment of the residual's independence with Moran's coefficient. The figure displayed below is a spatial autocorrelogram shows that there is spatial autocorrelation and therefore that the residuals are not independent.

```{r correlogram_1}
# Compute correlogram of the residuals
nbc <- 10
cor_r <- pgirmess::correlog(coords=spatial_reg[,c("lng", "lat")],
                            z=spatial_reg$residuals,
                            method="Moran", nbclass=nbc)

correlograms <- as.data.frame(cor_r)
correlograms$variable <- "residuals_glm" 

# Plot correlogram
ggplot(subset(correlograms, variable=="residuals_glm"), aes(dist.class, coef)) + 
  geom_hline(yintercept = 0, col="grey") +
  geom_line(col="steelblue") + 
  geom_point(col="steelblue") +
  xlab("distance") + 
  ylab("Moran's coefficient")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

An approach to account for the spatial structure could be to include the GPS coordinates as covariates in the model. For example

\(\eta_i = \beta_0 + \beta_1 x_i + \beta_2 s_{[i,1]} + \beta_3 s_{[i,2]}\),

where \((s_{[i,1]}, s_{[i,2]})\) are the longitude and latitude coordinates where each measurment was taken.

Having a trend across the surface may seem like a good idea, but it is not the best approach; and sometimes it is not even a good approach. First of all, the assumption of a surface trend may be to rigid. A polynomial fit, while more flexible, may still be too rigid or overfit the data. In any case we would need to decide which polynomial to use among all possibilities. For example

\(\eta_i = \beta_0 + \beta_1 x_i + \beta_2 s_{[i,1]} + \beta_3 s_{[i,2]} + \beta_4 s_{[i,1]} s_{[i,2]}\),

\(\eta_i = \beta_0 + \beta_1 x_i + \beta_2 s_{[i,1]} + \beta_3 s_{[i,2]} + \beta_4 s_{[i,1]} s_{[i,2]} + \beta_5 s_{[i,1]}^2 + \beta_6 s_{[i,2]}^2\),

etc...

## Spatial Covariance

The core idea behind Spatial Statistics is to understand an characterize this spatial dependence that is observed in different processes, for example: amount of rainfall, global temperature, air pollution, etc. Spatial Statistics deal with problems were nearby things are expected to be more alike.

When want to measure how much two variables change together, we use the covariance function. Under the right assumptions, we can also use the covariance function to describe the similarity of the observed values based on their location.

A covariance function *K*‚ÄÑ:‚ÄÑùïä‚ÄÖ√ó‚ÄÖùïä‚ÄÑ‚Üí‚ÄÑ‚Ñù maps a pair of points *z*<sub>1</sub>‚ÄÑ=‚ÄÑ(*s*<sub>\[1,‚ÄÜ1\]</sub>,‚ÄÜ*s*<sub>\[1,‚ÄÜ2\]</sub>) and *z*<sub>2</sub>‚ÄÑ=‚ÄÑ(*s*<sub>\[2,‚ÄÜ1\]</sub>,‚ÄÜ*s*<sub>\[2,‚ÄÜ2\]</sub>) to the real line. We can define such a function in terms of the distance between a pair of points. Let the distance between the points be given by *r*‚ÄÑ=‚ÄÑ‚à•*z*<sub>1</sub>‚ÄÖ‚àí‚ÄÖ*z*<sub>2</sub>‚à•, the following are examples of covarinace functions:

Exponentiated Quadratic: *K*(*z*<sub>1</sub>,‚ÄÜ*z*<sub>2</sub>)=*œÉ*<sup>2</sup>exp(‚àí*r*<sup>2</sup>/*œÅ*<sup>2</sup>)

Rational Quadratic: *K*(*z*<sub>1</sub>,‚ÄÜ*z*<sub>2</sub>)=*œÉ*<sup>2</sup>(1‚ÄÖ+‚ÄÖ*r*<sup>2</sup>/(2*Œ±**œÅ*<sup>2</sup>))<sup>‚àí*Œ±*</sup>

Matern Covariance: *K*(*z*<sub>1</sub>,‚ÄÜ*z*<sub>2</sub>)=*œÉ*<sup>2</sup>2<sup>1‚ÄÖ‚àí‚ÄÖ*ŒΩ*</sup>/*Œì*(*ŒΩ*)((2*ŒΩ*)<sup>.5</sup>*r*/*œÅ*)<sup>*ŒΩ*</sup>ùí¶<sub>*ŒΩ*</sub>((2*ŒΩ*)<sup>.5</sup>*r*/*œÅ*)

The quantities *œÅ*,‚ÄÜ*Œ±*,‚ÄÜ*ŒΩ* are parameters of the functions mentioned and ùí¶<sub>*ŒΩ*</sub> is the modified Bessel function of second kind. In the three cases, while less clear in the Matern case, the covariance decreases asymptotically towards zero the larger the value of *r*. This is the more distance between a pair of points, the weaker the covariance between them.

The election of which covariance function to use depends on our assumptions about the change in the association between the points across space (eg., the speed of decay). 

## Geostatistics

Now that we have discussed how the covariance function can help model spatial dependence, we can discuss how to incorporate this ideas into our model. In our GLM example above we fitted a model of the form

\(\eta_i = \beta_0 + \beta_1 x_i\),

Now we will incorporate an spatial component as

\(\eta_i = \beta_0 + \beta_1 x_i + f(z_i) \),
where  (\(f(z_1), \ldots , f(z_2) \)) is a multivariate Gaussian with spatial covariance \(K\).

We can implement this model, assuming a Matern covariance, as shown below.
```{r gam_spatial}
# Fit GAM with spatial smooth
m4 <- spaMM::fitme(OM ~ humidity + Matern(1|lng+lat), data=spatial_reg, family=gaussian(link="log"), init=list(lambda=.5, phi=.5))

summary(m4)
```
`nu` (*ŒΩ*) represents the 'smoothness' parameter and `rho` (œÅ) the scale parameter. `lambda` is the estimated variance in the random effect and `phi` the estimated variance in the residual error.

In the next figure, we will show the spatial effect by predicting the values of organic matter across space with a fixed level of humidity.
```{r spatial_grid}
# Make predictions on a spatial grid
surf_grid <- as.data.frame(make_grid(size = 20))
surf_grid$humidity <- mean(spatial_reg$humidity) # Assume covariate is constant across all space (for visualization only)
surf_grid$spatial_effect <- predict(m4, newdata=surf_grid, type="response")[, 1]
```

```{r plt_smooth, echo=FALSE}
# Plot smooth surface
ggplot(surf_grid, aes(lng, lat)) + 
  geom_raster(aes(fill=spatial_effect)) +
  geom_contour(aes(z=spatial_effect), col="white", linetype=1, alpha=.5) +
  viridis::scale_fill_viridis(option="plasma", na.value="darkblue")# + theme_void()
```

Next, we will compare the autocorrelation observed in the residuals of this geostatistic model and the autocorrelation of the residuals of the GLM. As we saw above, the residuals of the GLM were spatially correlated. That is not the case for the geostatistic model.
```{r correlogram_2}
# Compute correlogram of the residuals
cor_g <- pgirmess::correlog(coords=spatial_reg[,c("lng", "lat")],
                            z=residuals(m4),
                            method="Moran", nbclass=nbc)

cor_g <- as.data.frame(cor_g)
cor_g$variable <- "residuals_geostatistic"
correlograms <- rbind(correlograms, cor_g)

# Plot both correlograms
ggplot(correlograms, aes(dist.class, coef)) + 
  geom_hline(yintercept = 0, col="grey") +
  geom_line(aes(col=variable)) + 
  geom_point(aes(col=variable)) +
  xlab("distance") + 
  ylab("Moran's coefficient") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

## Cross-Validation

Cross-validation can be used for for model selection. Once we have different models with their parameters calibrated, we can use cross-valiation to select the one that has a better performance in the data of interest.

The proceedure for doing \(k\)-fold cross-validation is as follows. Instead of fitting the model to the observed data, we first split the data into \(k\) subsets. Then we train the model \(k\) times, each one using only 4 of the groups, and computing some performance metric on the left out group. The performance metric could be the sum of squared errors or any other sensible metric depending on the application. Below we show a diagram of a 5-fold cross validation.
```{r k_fold_cv, echo=FALSE}
# K-fold validation diagram
k = 5
label_ <- rep("Train", k^2)
for(i in 1:k) {
    label_[k*(i-1) +(k+1-i)] <- "Validation"
}
cvplot <- data.frame(x=rep(1:k, k), y=sort(rep(1:k, k), decreasing = TRUE), label=label_)

ggplot(cvplot, aes(x, y)) + geom_tile(aes(fill=label)) + 
  geom_vline(xintercept = 0:k + .5, col='white') +
  geom_hline(yintercept = 0:k + .5, col="white", size=10) +
  scale_y_continuous(breaks = 1:k, labels = paste("fold", 1:k)) +
  scale_x_continuous(breaks = 1:k, labels = paste("subset", 1:k)) +
  ggtitle(paste0(k, "-Fold Validation Diagram")) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank()) +
  ylab('') + xlab('')
```

### GLM Surface Trend vs Geostatistic Model

Before introducing the geostatistic models we discussed fitting a GLM using the location of the measurments as covariates. Here we will compare the performance of a GLM with a surface trend with a Geostatistic model using cross-validation.

The code below will split the data using only 3 folds.
```{r split}
# Copy spatial_reg without residuals
all_data <- spatial_reg[, c("lng", "lat", "humidity", "OM")]

# Make an index with the 2 folds
ix = caret::createFolds(all_data$OM, k = 3)
```

Now we will do 3-fold cross-validation using this the mean squared error as performance metric.
```{r cv_mse}
mse_glm <- c()
mse_geo <- c()
for (i in 1:3) {
  test_set <- all_data[ix[[i]], ] 
  train_set <- all_data[(1:300)[-c(ix[[i]])], ]
  m_glm <- glm(OM ~ humidity + lng + lat + lng*lat, data=train_set, family=gaussian(link="log"))
  m_geo <- spaMM::fitme(OM ~ humidity + Matern(1|lng+lat), data=train_set, family=gaussian(link="log"), init=list(lambda=.5, phi=.5))

  mse_glm[i] <- mean((predict(m_glm, newdata=test_set, type="response") - test_set$OM)^2)
  mse_geo[i] <- mean((predict(m_geo, newdata=test_set, type="response") - test_set$OM)^2)
}

print(mse_glm) # MSE for GLM in each round
print(mse_geo) # MSE for geostatistic model in each round

print(mean(mse_glm)) # Average MSE with GLM
print(mean(mse_geo)) # Average MSE with geostatistic model
```

Clearly the geostatisc model showed a better performance than a 1st order surface trend.

## Application Example: Malaria Case

Now we will estimate the prevalence of malaria in Oromoia State, Ethiopia, using data from 2009.
We have used this dataset in previous sessions.
This survey data contains information about number of positive cases and number of examined people in different schools. Spatial information is encoded in the fields *longitude* and *latitude*. To represent this data we can use a Binomial likelihood, that models number of successes out of a number of trials.
```{r ETH_malaria_data_data}
# Load data
ETH_malaria_data <- read.csv("https://raw.githubusercontent.com/HughSt/HughSt.github.io/master/course_materials/week1/Lab_files/Data/mal_data_eth_2009_no_dups.csv", header=T) # Case data
ETH_Adm_1 <- raster::getData("GADM", country="ETH", level=1) # Admin boundaries
Oromia <- subset(ETH_Adm_1, NAME_1=="Oromia")
# Plot both country and data points
raster::plot(Oromia)
points(ETH_malaria_data$longitude, ETH_malaria_data$latitude,
       pch = 16, ylab = "Latitude", xlab="Longitude", col="red", cex=.5)
```

To model and predict malaria prevalence across Oromia State, we need to first obtain predictors as rasters at a common resolution/extent. In this example, we are going to use two of the [Bioclim](https://www.worldclim.org/bioclim) layers, accessible using the `getData` function in the raster package. 

```{r}
bioclim_layers <- getData('worldclim', var='bio', res=0.5, lon=38.7578, lat=8.9806) # lng/lat for Addis Ababa
```
We can crop these layers to make them a little easier to handle
```{r}
bioclim_layers_oromia <- crop(bioclim_layers, Oromia)
plot(bioclim_layers_oromia[[1]]) # Bio1 - Annual mean temperature
lines(Oromia)
```

Now let's extract Bio1 (Annual mean temperature) and Bio12 (Annual precipitation) at the observation points
```{r}
ETH_malaria_data$bioclim1 <- extract(bioclim_layers_oromia[[1]], ETH_malaria_data[,c("longitude", "latitude")])
ETH_malaria_data$bioclim12 <- extract(bioclim_layers_oromia[[12]], ETH_malaria_data[,c("longitude", "latitude")])
```

Now we fit the model without a spatial effect
```{r}
prev_eth_non_spatial <- spaMM::fitme(cbind(pf_pos, examined - pf_pos) ~ bioclim1 + bioclim12, data=ETH_malaria_data, family=binomial())
summary(prev_eth_non_spatial)
```

And take a look at spatial autocorrelation in the residuals
```{r}
# Compute correlogram of the residuals
nbc <- 10
cor_r <- pgirmess::correlog(coords = ETH_malaria_data[,c("longitude", "latitude")],
                            z = residuals(prev_eth_non_spatial),
                            method="Moran", nbclass=nbc)
# Take a look
cor_r

# Plot correlogram
correlograms <- as.data.frame(cor_r)
correlograms$variable <- "residuals_glm" 

ggplot(subset(correlograms, variable=="residuals_glm"), aes(dist.class, coef)) + 
  geom_hline(yintercept = 0, col="grey") +
  geom_line(col="steelblue") + 
  geom_point(col="steelblue") +
  xlab("distance") + 
  ylab("Moran's coefficient")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), 
        panel.background = element_blank(), axis.line = element_line(colour = "black"))
```

There does appear to be residual spatial autocorrelation, so let's fit a spatial model.

```{r ETH_malaria_data_model}
prev_eth <- spaMM::fitme(cbind(pf_pos, examined - pf_pos) ~ bioclim1 + bioclim12 + Matern(1|latitude+longitude), data=ETH_malaria_data, family=binomial())
summary(prev_eth)
```

We can generate a quick scatter plot of observed vs fitted values
```{r}
plot(ETH_malaria_data$pf_pr, predict(prev_eth))
abline(0,1)
```

We have two bioclimatic variables. Are we sure we need these in the model? If not, how to decide which one(s) we should remove?

In this case, we are particularly interested in prediction, i.e. predicting prevalence values at un-surveyed locations. So, we will do 5-fold cross-validation using the mean squared error as performance metric. The function below will compute the mean square error across all folds. This function takes as input a dataset, an R formula object (that tells which covariates are used in the model) and a list of indices that describes the data splitting k folds.
```{r cv_eth}
# Function to compute a cross-validated MSE score
cv_eth <- function(data, spamm_formula, ix_test_list) {
  mse <- c()
  for (i in 1:length(ix_test_list)) {
    test_set <- data[ix_test_list[[i]], ] 
    train_set <- data[(1:300)[-c(ix_test_list[[i]])], ]
    model <- spaMM::fitme(spamm_formula, data=train_set, family=binomial())
    model_prediction <- predict(model, newdata=test_set, type="response")[,1]
    mse[i] <- mean((model_prediction * test_set$examined  - test_set$pf_pos)^2)
  }
  return(mean(mse))
}
```
  
As we saw above, to split the data into folds we call the command
```{r cv_eth_folds}
# Define 5 folds
ix = caret::createFolds(ETH_malaria_data$pf_pos, k = 5)
```

The procedure we will follow to choose the variables to include in the model is know as Backward Selection.
We start with a model that contains all variables and compute the CV-MSE. We then remove one variable at a time, refit the model and compute the CV-MSE. If the best of these new models  (with one less variable) outperforms the model with all variables, then select this new model as the optimal. Hence we will have decided which variable to remove. Afterwards we repeat the same proceedure removing a new variable from the ones that are still included in the new model.
The code to carry on the Backward Selection method is below.
```{r backwards_selection}
layer_names <- c("bioclim1", "bioclim12")
formula_kern <- "cbind(pf_pos, examined - pf_pos) ~ Matern(1|latitude+longitude)"
formula_model <- paste(c(formula_kern, layer_names), collapse = " + ")
scores <- c(cv_eth(ETH_malaria_data, as.formula(formula_model), ix))
# Simpler model
num_covariates <- length(layer_names)
max_covariates <- num_covariates - 1
indices <- 1:num_covariates

board <- data.frame(MSE = tail(scores, n=1), Covariates = paste(indices, collapse = ","))
while (num_covariates > max_covariates) {
  scores_iter <- c()
  ix_subsets <- gtools::combinations(n=num_covariates, r=num_covariates-1, v=indices)
  for (i in 1:nrow(ix_subsets)) {
    cov_subset <- layer_names[ix_subsets[i,]]
    formula_model <- paste(c(formula_kern, cov_subset), collapse = " + ")
    scores_iter <- c(scores_iter, cv_eth(ETH_malaria_data, as.formula(formula_model), ix))
  }
  best <- which.min(scores_iter)
  indices <- ix_subsets[best, ]
  scores <- c(scores, scores_iter[best])
  num_covariates <- length(indices)
  if (diff(tail(scores, n=2)) < 0 & max_covariates >= 2) {
    max_covariates <- max_covariates - 1
  }
  board <- rbind(board,
                 data.frame(MSE = tail(scores, n=1),
                            Covariates = paste(indices, collapse = ",")))
}

```

Here is a summary of the results. The covariates are indexed as: 1 - bioclim1, 2 - bioclim12.
```{r backwards_selection_results}
print(board)
```
According to this results the best model according to the lowest MSE is achieved when using both bioclim1 and bioclim12.

### Prediction
Now we have a model that relates our climatic layers to prevalence, we can predict the probability/prevalence of infection at any location within the region our data are representative where we have values of these climatic layers. It is possible to predict from a model directly onto a raster stack of covariates which makes life easy. However, in this case, we are using a geostatistical model, which includes latitude and longitude, and therefore we need to generate rasters of these to add to the stack of bioclim1 and bioclim12.
```{r}
# Create an empty raster with the same extent and resolution as the bioclimatic layers
latitude_raster <- longitude_raster <-raster(nrows = nrow(bioclim_layers_oromia[[1]]),
                                       ncols = ncol(bioclim_layers_oromia[[1]]),
                                                    ext = extent(bioclim_layers_oromia[[1]]))

# Change the values to be latitude and longitude respectively
longitude_raster[] <- coordinates(longitude_raster)[,1]
latitude_raster[] <- coordinates(latitude_raster)[,2]

# Now create a final prediction stack of the 4 variables we need
pred_stack <- stack(bioclim_layers_oromia[[c(1,12)]],
                    longitude_raster,
                    latitude_raster)

# Rename to ensure the names of the raster layers in the stack match those used in the model
names(pred_stack) <- c("bioclim1", "bioclim12", "longitude", "latitude")
plot(pred_stack)
```

Now we have a stack of rasters of the 4 variables used in the model at the same resolution and extent, we can run the `predict` function on the stack to produce a raster of preditions.

```{r}
predicted_prevalence_raster <- predict(pred_stack, prev_eth)
plot(predicted_prevalence_raster)
lines(Oromia)

# If you want to clip the predictions to Oromia
predicted_prevalence_raster_oromia <- mask(predicted_prevalence_raster, Oromia)
plot(log(predicted_prevalence_raster_oromia), col = map_palette("bruiser", 20))
```


## Conclusion

In this session we learnt the basic concepts of spatial regression. We saw how the spatial covariance is an essential component of a spatial model. By encoding the spatial association into a kernel function, a geostatistic model outperforms linear models even when they include a polynomial representation of the observations coordinates. When properly accounting for the spatial structure of the data, the residuals of the model are independent.

We also reviewed the concept of cross-validation as a means to select model. In particular we saw how it can help determine which covariates to include in the model.

## Pop quiz
Try adding elevation, land cover and one other variable to the model to see whether these improve the predictive accuracy. Remember to ensure your covariates are the same extent and resolution. Also, if you are using a categorical variable, like land cover, you have to ensure that all categories present in the prediction raster are included in the model. i.e. you can't make a prediction in a 'desert' pixel if none of your observations were made in 'desert'. 

## Assignment
Using [this](https://raw.githubusercontent.com/HughSt/HughSt.github.io/master/course_materials/week6/assignment/Data/BCG_vaccination_UGA.csv) dataset of BCG vaccination prevalence in Uganda in 2016, fit a model and generate maps of predicted prevalence of vaccination. Ensure that you check for residual spatial autocorrelation. Among other variables, you might be interested in using nighttime light intensity (often used as a proxy for economic activity/poverty) [this](https://geodata.globalhealthapp.net/). [This](https://forobs.jrc.ec.europa.eu/products/gam/download.php) dataset on travel times to nearest large city may also be useful (be aware this is a large file). Refer back to week 2 if you want to include distance to something (e.g. health facility etc.). 
